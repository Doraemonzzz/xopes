# Todo
- [ ] Additive Recurrence
  - [ ] Baseline
  - [ ] Recurrence Triton
    - [ ] fwd
    - [ ] bwd
  - [ ] Block Recurrence Triton
    - [ ] fwd
    - [ ] bwd
  - [ ] Block Parallel Triton
    - [ ] fwd
    - [ ] bwd
- [ ] LogCumsumExp
  - [ ] Recurrence Triton
    - [x] fwd
  - [ ] Block Recurrence Triton
    - [x] fwd
  - [ ] Block Parallel Triton
    - [x] fwd
- [x] Lrpe
  - [x] Cosine Triton
    - [x] 1d
      - [x] Readme
      - [x] fwd
      - [x] bwd
      - [x] act
        - [x] none
        - [x] silu
        - [x] relu
        - [x] softmax, dim=-1
        - [x] softmax, dim=-2
      - [x] Add offset
      - [x] Auto config
      - [x] Fused act
    - [ ] md
      - [x] Readme
      - [x] Cosine Triton
        - [x] fwd
        - [x] bwd
        - [ ] act
          - [x] none
          - [x] silu
          - [x] relu
          - [x] softmax, dim=-1
          - [ ] softmax, dim=-2
        - [x] Add extra token support
        - [x] Auto config
      - [x] Cosine Cache Triton
        - [x] fwd
        - [x] bwd
        - [ ] act
          - [x] none
          - [x] silu
          - [x] relu
          - [x] softmax, dim=-1
          - [ ] softmax, dim=-2
        - [x] Add extra token support
        - [x] Auto config
- [ ] Tpe
  - [ ] Triton
- [ ] Fuse Linear Attention Output Gate (flao)
  - [x] Non causal
    - [x] Document
    - [x] Lao Torch
    - [x] Flao Torch
    - [x] Flao Triton
      - [x] fwd
      - [x] bwd (in torch since no speed advantage)
      - [x] autotune
    - [ ] Flao Left Product Triton
      - [ ] fwd
      - [ ] bwd
    - [ ] Fused act lrpe non causal version
      - [x] Document
      - [x] Al Torch
      - [x] Fal Torch
- [ ] Act
  - [x] Document
  - [x] Torch
    - [x] relu
    - [x] sigmoid
    - [x] silu
    - [x] none
    - [x] softmax
  - [x] Triton
    - [x] relu
    - [x] sigmoid
    - [x] silu
    - [x] none
    - [x] softmax
- [ ] Custom benchmark function
  - [ ] https://github.com/triton-lang/triton/blob/main/python/triton/testing.py
- [ ]
