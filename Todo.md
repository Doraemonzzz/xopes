# Todo
- [ ] Additive Recurrence
  - [ ] Baseline
  - [ ] Recurrence Triton
    - [ ] fwd
    - [ ] bwd
  - [ ] Block Recurrence Triton
    - [ ] fwd
    - [ ] bwd
  - [ ] Block Parallel Triton
    - [ ] fwd
    - [ ] bwd
- [ ] LogCumsumExp
  - [ ] Recurrence Triton
    - [x] fwd
  - [ ] Block Recurrence Triton
    - [x] fwd
  - [ ] Block Parallel Triton
    - [x] fwd
- [x] Lrpe
  - [x] Cosine Triton
    - [x] Document
    - [x] fwd
      - [x] fn
    - [x] bwd
      - [x] fn
    - [x] Add offset
    - [x] Auto config
    - [x] Fused act
- [ ] MdLrpe
  - [ ] Readme
  - [x] Cosine Triton
    - [x] fwd
      - [x] fn
    - [x] bwd
      - [x] fn
    - [x] Add extra token support
    - [x] Auto config
    - [ ] Fused act
  - [x] Cosine Cache Triton
    - [x] fwd
      - [x] fn
    - [x] bwd
      - [x] fn
    - [x] Add extra token support
    - [x] Auto config
- [ ] Tpe
  - [ ] Triton
- [ ] Fuse Linear Attention Output Gate (flao)
  - [x] Non causal
    - [x] Document
    - [x] Lao Torch
    - [x] Flao Torch
    - [x] Flao Triton
      - [x] fwd
      - [x] bwd (in torch since no speed advantage)
      - [x] autotune
    - [ ] Flao Left Product Triton
      - [ ] fwd
      - [ ] bwd
    - [ ] Fused act lrpe non causal version
      - [x] Document
      - [x] Al Torch
      - [x] Fal Torch
- [ ] Act
  - [x] Document
  - [x] Torch
    - [x] relu
    - [x] sigmoid
    - [x] silu
    - [x] none
    - [ ] softmax
  - [x] Triton
    - [x] relu
    - [x] sigmoid
    - [x] silu
    - [x] none
    - [x] softmax
- [ ] Custom benchmark function
  - [ ] https://github.com/triton-lang/triton/blob/main/python/triton/testing.py
- [ ]
